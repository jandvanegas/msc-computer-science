{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69b2fe7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea3949a9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1839) (1839,)\n",
      "(11, 1822) (1822,)\n",
      "(8, 1813)\n",
      "{'Sjoint': array([[4.71621403e+02, 8.30499915e-01, 5.63613936e-05, ...,\n",
      "        1.42739038e+02, 1.96840943e+01, 2.41729527e+01],\n",
      "       [1.74690046e+02, 4.62669093e-03, 2.05391687e-02, ...,\n",
      "        4.76768986e-04, 9.52666421e-05, 1.87064739e+02]]), 'SMarginal': array([[6.46311449e+02, 8.35126606e-01, 2.05955301e-02, ...,\n",
      "        1.42739515e+02, 1.96841895e+01, 2.11237692e+02]]), 'posterior': array([[7.29712283e-01, 9.94459893e-01, 2.73658378e-03, ...,\n",
      "        9.99996660e-01, 9.99995160e-01, 1.14434846e-01],\n",
      "       [2.70287717e-01, 5.54010721e-03, 9.97263416e-01, ...,\n",
      "        3.34013316e-06, 4.83975436e-06, 8.85565154e-01]]), 'SPost': array([0, 0, 1, ..., 0, 0, 1], dtype=int64), 'acc': 0.851627137341423, 'err': 0.14837286265857696}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "\n",
    "def mcol(v):\n",
    "    return v.reshape((v.size, 1))\n",
    "\n",
    "# Load of data function\n",
    "def load(fname):\n",
    "    DList = []\n",
    "    labelsList = []\n",
    "\n",
    "    with open(fname) as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                attrs = line.split(',')[0:11]\n",
    "                attrs = mcol(np.array([float(i) for i in attrs]))\n",
    "                label = line.split(',')[-1].strip()\n",
    "                DList.append(attrs)\n",
    "                labelsList.append(label)\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return np.hstack(DList), np.array(labelsList, dtype=np.int32)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ##Load train\n",
    "    X_train, y_train = load('Train.txt')\n",
    "    print(X_train.shape,y_train.shape)\n",
    "#     plot_hist(X_train, y_train)\n",
    "#     plot_scatter(X_train, y_train)\n",
    "    #Load test\n",
    "    X_test, y_test = load('Test.txt')\n",
    "    print(X_test.shape,y_test.shape)\n",
    "    \n",
    "    X_train,y_train,X_test,y_test = Preprocess_data(X_train,y_train,X_test,y_test,outlierK=9)\n",
    "    \n",
    "    classifier = 'mvg'\n",
    "    print(Compute_gaussian_model(X_train,y_train,X_test,y_test,classifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05016ea3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make outlier detection and feature selection\n",
    "###### The classes are ordered and not balanced (e.g. there are many more normal wines than excellent or poor ones). Outlier detection algorithms could be used to detect the few excellent or poor wines. Also, we are not sure if all input variables are relevant. So it could be interesting to test feature selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a1400",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dda000",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Preprocess_data(X_train,y_train,X_test,y_test,outlierK = 1.5):\n",
    "    \n",
    "    outliers = Outliers_IQR(X_train,outlierK)\n",
    "    X_train_sin_Out = np.delete(X_train,outliers,1)\n",
    "    y_train_sin_Out = np.delete(y_train,outliers,0)\n",
    "    \n",
    "    X_train_norm = standarization(X_train_sin_Out)\n",
    "    \n",
    "    for i in range (0,X_train_norm.shape[0]):\n",
    "        DP, accumulated, P = PCA(X_train_norm,i)\n",
    "        if(accumulated > 0.95):\n",
    "            m = i\n",
    "#             print(i)\n",
    "            break\n",
    "    \n",
    "    outliers = Outliers_IQR(X_test,outlierK)\n",
    "    X_test_sin_Out = np.delete(X_test,outliers,1)\n",
    "    y_test_sin_Out = np.delete(y_test,outliers,0)\n",
    "    \n",
    "    X_test_norm = standarization(X_test_sin_Out)\n",
    "    DP_test = np.dot(P.T,X_test_norm)\n",
    "    print(DP_test.shape)\n",
    "    \n",
    "    return DP,y_train_sin_Out,DP_test,y_test_sin_Out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bbbd5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Auxiliar pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9a2eef9",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "\n",
    "def Outliers_IQR(x,k):\n",
    "    \"\"\"\n",
    "    Compute the outliers based on IQR\n",
    "    :param x: matrix of features\n",
    "    :return: np array with the index of the outliers\n",
    "    \"\"\"\n",
    "    q1 = np.percentile(x,25, axis = 1)\n",
    "    q3 = np.percentile(x,75, axis = 1)\n",
    "    IQR = q3 - q1\n",
    "    IQR\n",
    "    outMayor = q3 + (k * IQR)\n",
    "    outMenor = q1 - (k *IQR)\n",
    "    \n",
    "    outliers = []\n",
    "    count = 0\n",
    "    for j in range (0,x.shape[1]):\n",
    "        for i in range (0,x.shape[0]):\n",
    "            if x[i,j]>outMayor[i] or x[i,j]<outMenor[i]:\n",
    "#                 print(j) #Tengo que eliminar los ejemplos que aparecen en j (unique)\n",
    "                outliers.append(j)\n",
    "                count+=1\n",
    "        \n",
    "    return np.unique(outliers)\n",
    "\n",
    "\n",
    "\n",
    "# Standardization of the data (fit them on the same range) since the ranges of the features are so different.\n",
    "\n",
    "def standarization(D):\n",
    "    \"\"\"\n",
    "    Compute the min-max standarization for the given data by rows\n",
    "    :param D: matrix of data \n",
    "    :param m: number of PCA to compute\n",
    "    :return: np array with the projection of the given data on the computed PCs\n",
    "    \"\"\"\n",
    "    standarized = np.zeros((D.shape[0],D.shape[1]))\n",
    "\n",
    "    for i in range (D.shape[0]):\n",
    "        for j in range (D.shape[1]):\n",
    "            standarized[i,j]= ((D[i][j]-D[i].min())/(D[i].max()-D[i].min()))\n",
    "    return standarized\n",
    "\n",
    "\n",
    "def PCA(D,m):\n",
    "    \"\"\"\n",
    "    Compute the PCA for the given dara\n",
    "    :param D: matrix of features\n",
    "    :param m: number of PCA to compute\n",
    "    :return: np array with the projection of the given data on the computed PCs\n",
    "    \"\"\"\n",
    "    \n",
    "# Extracting data mean\n",
    "    mu = mcol(D.mean(1))\n",
    "# Centering Data\n",
    "    DC = D - mu\n",
    "    C = np.dot(DC, DC.T)/ D.shape[1]\n",
    "# Compute eigenvectors and eogenvalues\n",
    "\n",
    "    U, s, Vt = np.linalg.svd(C)\n",
    "#     print(s)\n",
    "\n",
    "# We are going to use 2 PC and plot them\n",
    "    P = U[:, 0:m]\n",
    "    DP = np.dot(P.T, D)\n",
    "\n",
    "    covarianza1 = covariance(DP)\n",
    "    numerador = sum(covarianza1[i][i] for i in range(covarianza1.shape[1]))\n",
    "    \n",
    "    covarianzatotal = covariance(D)\n",
    "    denominador = sum(covarianzatotal[i][i] for i in range(covarianzatotal.shape[1]))\n",
    "    sumvarianze = (numerador/denominador)\n",
    "    # Podemos ver aqui que hay un ejemplo que es un outlier, debemos eliminar primero los outliers antes de aplicar PCA\n",
    "    return DP, sumvarianze, P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f839cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gaussian generative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f47932b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sjoint': array([[2.65993487e-02, 2.11713865e-04, 3.11686123e-07, ...,\n",
       "         8.82435609e-02, 4.82349015e-02, 2.05346202e-05],\n",
       "        [5.11706703e-02, 4.24100557e-07, 4.46636317e-07, ...,\n",
       "         1.22529918e-06, 1.27758330e-07, 7.06205411e-04]]),\n",
       " 'SMarginal': array([[7.77700190e-02, 2.12137965e-04, 7.58322440e-07, ...,\n",
       "         8.82447862e-02, 4.82350293e-02, 7.26740031e-04]]),\n",
       " 'posterior': array([[3.42025746e-01, 9.98000827e-01, 4.11020572e-01, ...,\n",
       "         9.99986115e-01, 9.99997351e-01, 2.82557989e-02],\n",
       "        [6.57974254e-01, 1.99917330e-03, 5.88979428e-01, ...,\n",
       "         1.38852303e-05, 2.64866285e-06, 9.71744201e-01]]),\n",
       " 'SPost': array([1, 0, 1, ..., 0, 0, 1], dtype=int64),\n",
       " 'acc': 0.8150384193194292,\n",
       " 'err': 0.18496158068057078}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Compute_gaussian_model(X_train,y_train,X_test,y_test,classifier):\n",
    "    \"\"\"\n",
    "    Compute different gaussian models\n",
    "    :param X_train: matrix of train features\n",
    "    :param y_train: matrix of train labels\n",
    "    :param X_test: matrix of test features\n",
    "    :param y_test: matrix of test labels\n",
    "    :classifier: ('mvg','naive','tied_naive'): gaussian model to be computed\n",
    "    :return: np array with the index of the outliers\n",
    "    \"\"\"\n",
    "    mean, covariance = compute_classifier(X_train, y_train, classifier=classifier)\n",
    "    evaluation = evaluate(X_test, y_test, mean, covariance,\n",
    "                              logarithmically=False)\n",
    "    \n",
    "    return(evaluation)\n",
    "\n",
    "\n",
    "classifier = 'mvg'\n",
    "Compute_gaussian_model(X_train,y_train,X_test,y_test,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f8a78",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Also we have to do (maybe not do but explain) that is not so useful to use LDA on binary classification problems according what professor said in teory class. (Only 1 discriminant direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adf2bdba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logpdf_GAU_ND' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-962b68be1442>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m# Computing MVG and Log-Likelihood\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[0mMVG\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlogpdf_GAU_ND\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDP\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcovariance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mMVG\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mLL\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloglikelihood\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDP\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mcovariance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mDP\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'logpdf_GAU_ND' is not defined"
     ]
    }
   ],
   "source": [
    "# Computing MVG and Log-Likelihood\n",
    "\n",
    "MVG = logpdf_GAU_ND(DP,mean(DP),covariance(DP))\n",
    "print(MVG)\n",
    "LL = loglikelihood(DP,mean(DP),covariance(DP))\n",
    "print(LL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3bc2c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Logistic regression (sin terminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18280fd0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-4718f1f2157c>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     36\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mlog_reg_func\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 38\u001B[1;33m \u001B[0mX_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDP\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     39\u001B[0m \u001B[0my_train\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my_train_sin_Out\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[0mX_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mDP_test\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'DP' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "def optimize_function_yz():\n",
    "    x, f, d = fmin_l_bfgs_b(function_yz,\n",
    "                            x0=np.array([0, 0]),\n",
    "                            approx_grad=False,\n",
    "                            iprint=1)\n",
    "    print(f\"x: {x}\")\n",
    "    print(f\"f: {f}\")\n",
    "    print(f\"d: {d}\")\n",
    "\n",
    "\n",
    "def function_yz(x: np.ndarray):\n",
    "    y = x[0]\n",
    "    z = x[1]\n",
    "    f = np.power(y + 3, 2) + np.sin(y) + np.power(z + 1, 2)\n",
    "    gradient = np.array([\n",
    "        2 * (y + 3) + np.cos(y),\n",
    "        2 * (z + 1)\n",
    "    ])\n",
    "    return f, gradient\n",
    "\n",
    "def log_reg_func_factory(DTR: np.ndarray,\n",
    "                         LTR: np.ndarray,\n",
    "                         lambda_param: float):\n",
    "    Z = 2 * LTR - 1\n",
    "    n = DTR.shape[1]\n",
    "\n",
    "    def log_reg_func(x):\n",
    "        w = prml.vcol(x[0:-1])\n",
    "        b = x[-1]\n",
    "        regularization = np.power(np.linalg.norm(w), 2) * lambda_param / 2.0\n",
    "        loss = np.sum(np.logaddexp(0, - Z * (w.T @ DTR + b))) / n\n",
    "        return regularization + loss\n",
    "\n",
    "    return log_reg_func\n",
    "\n",
    "X_train = DP\n",
    "y_train = y_train_sin_Out\n",
    "X_test = DP_test\n",
    "y_test = y_test_sin_Out\n",
    "\n",
    "def train_binary_regression(X_train,y_train,X_test,y_test):\n",
    "#     D, L = load_iris_binary()\n",
    "#     (DTR, LTR), (DTE, LTE) = prml.split_data(D, L, 2.0 / 3.0)\n",
    "    log_reg_func = log_reg_func_factory(X_train, y_train, lambda_param=0.01)\n",
    "    omegas, f, d = fmin_l_bfgs_b(log_reg_func,\n",
    "                                 x0=np.zeros(X_train.shape[0] + 1),\n",
    "                                 approx_grad=True,\n",
    "                                 iprint=1)\n",
    "    print(f\"omegas: {omegas}\")\n",
    "    print(f\"f: {f}\")\n",
    "    print(f\"d: {d}\")\n",
    "    w = omegas[0:-1]\n",
    "    b = omegas[-1]\n",
    "    prediction = prml.vcol(w).T @ X_test + b\n",
    "    score = (np.ones((1, X_test.shape[1])) * prediction > 0).astype(int)\n",
    "    print(f\"scores: {1 - np.sum(score == prml.vrow(y_test)) / X_test.shape[1]}\")\n",
    "    \n",
    "train_binary_regression(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86693aa2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Auxiliar functions to visualize data information graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2513f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function for plotting histograms of each feature\n",
    "def plot_hist(D, L):\n",
    "\n",
    "    D0 = D[:, L==0]\n",
    "    D1 = D[:, L==1]\n",
    "\n",
    "    hFea = {\n",
    "        0: 'Fixed acidity',\n",
    "        1: 'Volatile acidity',\n",
    "        2: 'Citric acid',\n",
    "        3: 'Petal width',\n",
    "        4: 'Residual sugar',\n",
    "        5: 'Chlorides',\n",
    "        6: 'Free sulfur dioxide',\n",
    "        7: 'Total sulfur dioxide',\n",
    "        8: 'Density',\n",
    "        9: 'pH',\n",
    "        10: 'Sulphates',\n",
    "        11: 'Alcohol'\n",
    "        }\n",
    "\n",
    "    for dIdx in range(11):\n",
    "        plt.figure()\n",
    "        plt.xlabel(hFea[dIdx])\n",
    "        plt.hist(D0[dIdx, :], bins = 10, density = True, alpha = 0.4, label = 'Good Wine')\n",
    "        plt.hist(D1[dIdx, :], bins = 10, density = True, alpha = 0.4, label = 'Bad Wine')\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.tight_layout() # Use with non-default font size to keep axis label inside the figure\n",
    "#         plt.savefig('hist_%d.pdf' % dIdx)\n",
    "    plt.show()\n",
    "\n",
    "# Function for plotting scatter of each pair os features\n",
    "def plot_scatter(D, L):\n",
    "    \n",
    "    D0 = D[:, L==0]\n",
    "    D1 = D[:, L==1]\n",
    "\n",
    "    hFea = {\n",
    "        0: 'Fixed acidity',\n",
    "        1: 'Volatile acidity',\n",
    "        2: 'Citric acid',\n",
    "        3: 'Petal width',\n",
    "        4: 'Residual sugar',\n",
    "        5: 'Chlorides',\n",
    "        6: 'Free sulfur dioxide',\n",
    "        7: 'Total sulfur dioxide',\n",
    "        8: 'Density',\n",
    "        9: 'pH',\n",
    "        10: 'Sulphates',\n",
    "        11: 'Alcohol'\n",
    "        }\n",
    "\n",
    "    for dIdx1 in range(11):\n",
    "        for dIdx2 in range(11):\n",
    "            if dIdx1 == dIdx2:\n",
    "                continue\n",
    "            plt.figure()\n",
    "            plt.xlabel(hFea[dIdx1])\n",
    "            plt.ylabel(hFea[dIdx2])\n",
    "            plt.scatter(D0[dIdx1, :], D0[dIdx2, :], label = 'Good Wine')\n",
    "            plt.scatter(D1[dIdx1, :], D1[dIdx2, :], label = 'Bad Wine')\n",
    "           \n",
    "            plt.legend()\n",
    "            plt.tight_layout() # Use with non-default font size to keep axis label inside the figure\n",
    "#             plt.savefig('scatter_%d_%d.pdf' % (dIdx1, dIdx2))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe23c7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## PRML Imported directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c666065",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "\n",
    "import prml\n",
    "\n",
    "\n",
    "def vcol(vector: np.ndarray):\n",
    "    \"\"\"\n",
    "    Converts vector into column vector\n",
    "    :param vector: one dimensional vector np.ndarray (size,)\n",
    "    :return: vector: two dimensional vector (size, 1)\n",
    "    \"\"\"\n",
    "    return vector.reshape((vector.size, 1))\n",
    "\n",
    "\n",
    "def vrow(vector: np.ndarray):\n",
    "    \"\"\"\n",
    "    Converts vector into row vector\n",
    "    :param vector: one dimensional vector np.ndarray (size,)\n",
    "    :return: vector: two dimensional vector (1, size)\n",
    "    \"\"\"\n",
    "    return vector.reshape((1, vector.size))\n",
    "\n",
    "\n",
    "def load(file_name: str):\n",
    "    \"\"\"\n",
    "    Return two np.ndarray objects with data and target values\n",
    "    File must be in the format\n",
    "    float, float, float, ..., str\n",
    "    Having first values as integers/float for the features and last value as\n",
    "    the target class\n",
    "    :param file_name: name of a csv file\n",
    "    :return: np.ndarray features x N , np.ndarray features,\n",
    "    \"\"\"\n",
    "    array = []\n",
    "    targets = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for raw_line in file:\n",
    "            if raw_line:\n",
    "                line = raw_line.split(',')\n",
    "                raw_values, target = line[:-1], line[-1]\n",
    "                values = [float(value) for value in raw_values]\n",
    "                array.append(values)\n",
    "                targets.append(target.strip())\n",
    "    return np.array(array).T, np.array(targets)\n",
    "\n",
    "\n",
    "def mean(matrix_d):\n",
    "    \"\"\"\n",
    "    Compute mean of a matrix_d\n",
    "    :param matrix_d: features x N data matrix\n",
    "    :return: features x 1 column vector\n",
    "    \"\"\"\n",
    "    return vcol(matrix_d.mean(1))\n",
    "\n",
    "\n",
    "def covariance(matrix_d):\n",
    "    \"\"\"\n",
    "    Compute covariance of a np.ndarray features x N\n",
    "    :param matrix_d: features x N\n",
    "    :return: covariance matrix : features x features\n",
    "    \"\"\"\n",
    "    N = matrix_d.shape[1]\n",
    "    mu = mean(matrix_d)\n",
    "    data_centered = (matrix_d - mu)\n",
    "    return data_centered @ data_centered.T / N\n",
    "\n",
    "\n",
    "def covariance_between(matrix_d, matrix_l):\n",
    "    \"\"\"\n",
    "    Return covariance between classes\n",
    "    :param matrix_d: features x N\n",
    "    :param matrix_l: classes vector\n",
    "    :return: covariance matrix features x features\n",
    "    \"\"\"\n",
    "    classes = set(matrix_l)\n",
    "    features = matrix_d.shape[0]\n",
    "    N = matrix_d.shape[1]\n",
    "    s_b = np.zeros((features, features))\n",
    "    mu = mean(matrix_d)\n",
    "    for class_l in classes:\n",
    "        d_class = matrix_d[:, matrix_l == class_l]\n",
    "        nc = d_class.shape[1]\n",
    "        mu_c = mean(d_class)\n",
    "        classes_distance = mu_c - mu\n",
    "        summation = np.multiply(nc, classes_distance) @ classes_distance.T\n",
    "        s_b = s_b + summation\n",
    "    return s_b / N\n",
    "\n",
    "\n",
    "def covariance_within(matrix_d, matrix_l):\n",
    "    classes = set(matrix_l)\n",
    "    N = matrix_d.shape[1]\n",
    "    features = matrix_d.shape[0]\n",
    "    s_w = np.zeros((features, features))\n",
    "    for class_l in classes:\n",
    "        d_class = matrix_d[:, matrix_l == class_l]\n",
    "        mu_c = mean(d_class)\n",
    "        central_data = d_class - mu_c\n",
    "        class_summation = central_data @ central_data.T\n",
    "        s_w = s_w + class_summation\n",
    "    return s_w / N\n",
    "\n",
    "\n",
    "def compute_w(s_b, s_w, m):\n",
    "    s, U = scipy.linalg.eigh(s_b, s_w)\n",
    "    W = U[:, ::-1][:, 0:m]\n",
    "    return W\n",
    "\n",
    "\n",
    "def eigh(matrix_d):\n",
    "    \"\"\"\n",
    "    Return eigen values and vectors using np. linalg.eigh but in desc order\n",
    "    :param matrix_d: symmetric matrix\n",
    "    :return: np.ndarray with eigen values, np.ndarray with eigen vectors\n",
    "    \"\"\"\n",
    "    eig_values, eig_vectors = np.linalg.eigh(matrix_d)\n",
    "    return eig_values[::-1], eig_vectors[:, ::-1]\n",
    "\n",
    "\n",
    "def compute_pca(matrix_d, m):\n",
    "    \"\"\"\n",
    "    Computes the Principal component Analysis of a Matrix\n",
    "    :param matrix_d: (np.ndarray features, N) matrix\n",
    "    :param m: number of components\n",
    "    :return: Data projected over the m principal components\n",
    "    \"\"\"\n",
    "    d_covariance = covariance(matrix_d)\n",
    "    eig_values, eig_vectors = eigh(d_covariance)\n",
    "    P = eig_vectors[:, 0:m]  # Eigen vectors to project\n",
    "    DP = P.T @ matrix_d  # Matrix D projected on P\n",
    "    return DP\n",
    "\n",
    "\n",
    "def compute_w_2(s_b, s_w, m):\n",
    "    U, s, _ = np.linalg.svd(s_w)\n",
    "    P1 = np.dot(U * vrow(1.0 / (s ** 0.5)), U.T)\n",
    "    Sbt = P1 @ s_b @ P1.T\n",
    "    _, P2 = eigh(Sbt)\n",
    "    P2 = P2[:, 0:m]\n",
    "    W = P1.T @ P2\n",
    "    return W\n",
    "\n",
    "\n",
    "def compute_lda(matrix_d, targets, m):\n",
    "    S_b = covariance_between(matrix_d, targets)\n",
    "    S_w = covariance_within(matrix_d, targets)\n",
    "    W = compute_w_2(S_b, S_w, m)\n",
    "    DP = W.T @ matrix_d\n",
    "    return DP\n",
    "\n",
    "\n",
    "def logpdf_GAU_ND(X, mu, C):\n",
    "    \"\"\"\n",
    "    Computes the Multivariate Gaussian Density\n",
    "    :param X: matrix features x samples\n",
    "    :param mu: mean\n",
    "    :param C: empirical covariance\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    M = X.shape[0]\n",
    "    first_term = - 0.5 * M * np.log(2 * math.pi)\n",
    "    centered_x = X - mu\n",
    "    second_term = - 0.5 * np.linalg.slogdet(C)[1]\n",
    "    third_term = - 0.5 * np.sum(\n",
    "        (centered_x.T @ np.linalg.inv(C)) * centered_x.T,\n",
    "        axis=1)\n",
    "    return first_term + second_term + third_term\n",
    "\n",
    "\n",
    "def loglikelihood(X, m, C):\n",
    "    return logpdf_GAU_ND(X, m, C).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d29c170",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_50_50(classifier):\n",
    "    D, L = load_iris()\n",
    "    (DTR, LTR), (DTE, LTE) = split_db_2to1(D, L)\n",
    "    mean, covariance = compute_classifier(DTR, LTR, classifier=classifier)\n",
    "    return evaluate(DTE, LTE, mean, covariance, logarithmically=False)\n",
    "\n",
    "\n",
    "def compute_classifier(DTR, LTR, classifier):\n",
    "    data_by_classes = split_by_classes(DTR, LTR)\n",
    "    mean_by_class, covariance_by_class = get_mean_and_covariance(\n",
    "        data_by_classes)\n",
    "    classifier_covariance = compute_classifier_covariance(covariance_by_class,\n",
    "                                                          classifier, DTR, LTR)\n",
    "    return mean_by_class, classifier_covariance\n",
    "\n",
    "\n",
    "def compute_classifier_covariance(covariance, classifier, D, L):\n",
    "    if classifier == 'mvg':\n",
    "        return covariance\n",
    "    elif classifier == 'naive':\n",
    "        return diagonalize_covariance(covariance)\n",
    "    elif classifier == 'tied':\n",
    "        return [prml.covariance_within(D, L) for _ in range(3)]\n",
    "    elif classifier == 'tied_naive':\n",
    "        # TODO validate this\n",
    "        covariance = prml.covariance(D)\n",
    "        return diagonalize_covariance([covariance for _ in range(3)])\n",
    "\n",
    "\n",
    "def evaluate(DTE, LTE, mean, covariance, logarithmically):\n",
    "    S = get_score_matrix(DTE, mean, covariance,\n",
    "                         logarithmically=logarithmically)\n",
    "    prior = np.array([[np.count_nonzero(y_train == 0)/ y_train.shape[0]], [np.count_nonzero(y_train == 1)/ y_train.shape[0]]])\n",
    "    SJoint = compute_join(S, prior, logarithmically=logarithmically)\n",
    "    SMarginal = compute_marginal(SJoint, logarithmically=logarithmically)\n",
    "    posterior = compute_posterior(SJoint, SMarginal,\n",
    "                                  logarithmically=logarithmically)\n",
    "    SPost = np.argmax(posterior, axis=0)\n",
    "    accuracy = np.sum(SPost == LTE) / DTE.shape[1]\n",
    "    err = 1.0 - accuracy\n",
    "    return {\n",
    "        'Sjoint': SJoint,\n",
    "        'SMarginal': SMarginal,\n",
    "        'posterior': posterior,\n",
    "        'SPost': SPost,\n",
    "        'acc': accuracy,\n",
    "        'err': err\n",
    "    }\n",
    "\n",
    "\n",
    "def diagonalize_covariance(covariance_by_class):\n",
    "    diagonalized_covariances = []\n",
    "    for covariance in covariance_by_class:\n",
    "        size = covariance.shape[0]\n",
    "        diagonalized_covariances.append(covariance * np.identity(size))\n",
    "    return diagonalized_covariances\n",
    "\n",
    "\n",
    "def compute_posterior(SJoint, SMarginal, logarithmically=False):\n",
    "    if logarithmically:\n",
    "        return np.exp(SJoint - SMarginal)\n",
    "    return SJoint / SMarginal\n",
    "\n",
    "\n",
    "def compute_marginal(SJoint, logarithmically=False):\n",
    "    if logarithmically:\n",
    "        return prml.vrow(scipy.special.logsumexp(SJoint, axis=0))\n",
    "    return prml.vrow(SJoint.sum(0))\n",
    "\n",
    "\n",
    "def get_score_matrix(samples, mean_by_class, covariance_by_class,\n",
    "                     logarithmically: False):\n",
    "    samples_number = samples.shape[1]\n",
    "    score = np.empty((0, samples_number))\n",
    "    for mean, covariance in zip(mean_by_class, covariance_by_class):\n",
    "        class_score = prml.logpdf_GAU_ND(samples, mean, covariance)\n",
    "        if not logarithmically:\n",
    "            class_score = np.exp(class_score)\n",
    "        score = np.vstack([score, class_score])\n",
    "    return score\n",
    "\n",
    "\n",
    "def compute_join(score, class_probability, logarithmically=False):\n",
    "    if logarithmically:\n",
    "        return score + np.log(class_probability)\n",
    "    return class_probability * score\n",
    "\n",
    "\n",
    "def load_iris():\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    D, L = iris['data'].T, iris['target']\n",
    "    return D, L\n",
    "\n",
    "\n",
    "def get_mean_and_covariance(data_by_classes):\n",
    "    mean = []\n",
    "    covariance = []\n",
    "    for class_data in data_by_classes:\n",
    "        mean.append(prml.mean(class_data))\n",
    "        covariance.append(prml.covariance(class_data))\n",
    "    return mean, covariance\n",
    "\n",
    "\n",
    "def split_by_classes(data, labels):\n",
    "    classes = []\n",
    "    for _class in set(labels):\n",
    "        classes.append(data[:, labels == _class])\n",
    "    return classes\n",
    "\n",
    "\n",
    "def split_db_2to1(D: np.ndarray, L: np.ndarray, seed=0):\n",
    "    nTrain = int(D.shape[1] * 2.0 / 3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "\n",
    "    DTR = D[:, idxTrain]\n",
    "    DTE = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LTE = L[idxTest]\n",
    "    return (DTR, LTR), (DTE, LTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088c5a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e1df2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}